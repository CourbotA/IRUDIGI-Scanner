{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports that will be used to construct the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model,Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import * \n",
    "from tensorflow_addons.layers import InstanceNormalization\n",
    "from glob import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "dataset_name = 'apple2orange'\n",
    "\n",
    "DOWNLOAD_URL = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/{}.zip'.format(dataset_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators using the U-net architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_generator(img_shape,channels=3,num_filters=64):\n",
    "    \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "    def upsample_block(incoming_layer,skip_input_layer,num_filters,kernel_size=4,dropout_rate=0):\n",
    "        \"\"\"Layers used during upsampling\"\"\"\n",
    "        upsample_layer = UpSampling2D(size=2)(incoming_layer)\n",
    "        upsample_layer = Conv2D(num_filters,kernel_size=kernel_size,strides=1,padding='same',activation='relu')(upsample_layer)\n",
    "        if dropout_rate:\n",
    "            upsample_layer = Dropout(dropout_rate)(upsample_layer)\n",
    "            upsample_layer = BatchNormalization(momentum=0.8)(upsample_layer)\n",
    "            upsample_layer = Concatenate()([upsample_layer, skip_input_layer])\n",
    "        return upsample_layer   \n",
    "    \n",
    "    def downsample_block(incoming_layer,num_filters,kernel_size=4,batch_normalization=True):\n",
    "        \"\"\"Layers used during downsampling\"\"\"\n",
    "        downsample_layer = Conv2D(num_filters,kernel_size=kernel_size,strides=2, padding='same')(incoming_layer)\n",
    "        downsample_layer = LeakyReLU(alpha=0.2)(downsample_layer)\n",
    "        if batch_normalization:\n",
    "            downsample_layer = BatchNormalization(momentum=0.8)(downsample_layer)\n",
    "        return downsample_layer\n",
    "\n",
    "    # Image input\n",
    "    input_layer = Input(shape=img_shape)\n",
    "    # Downsampling\n",
    "    down_sample_1 = downsample_block(input_layer,\n",
    "    num_filters,\n",
    "    batch_normalization=False)\n",
    "    # rest of the downsampling blocks have batch_normalization=true\n",
    "    down_sample_2 = downsample_block(down_sample_1, num_filters*2)\n",
    "    down_sample_3 = downsample_block(down_sample_2, num_filters*4)\n",
    "    down_sample_4 = downsample_block(down_sample_3, num_filters*8)\n",
    "    down_sample_5 = downsample_block(down_sample_4, num_filters*8)\n",
    "    down_sample_6 = downsample_block(down_sample_5, num_filters*8)\n",
    "    down_sample_7 = downsample_block(down_sample_6, num_filters*8)\n",
    "    # Upsampling blocks with skip connections\n",
    "    upsample_1 = upsample_block(down_sample_7, down_sample_6,num_filters*8)\n",
    "    upsample_2 = upsample_block(upsample_1, down_sample_5,num_filters*8)\n",
    "    upsample_3 = upsample_block(upsample_2, down_sample_4,num_filters*8)\n",
    "    upsample_4 = upsample_block(upsample_3, down_sample_3,num_filters*8)\n",
    "    upsample_5 = upsample_block(upsample_4, down_sample_2,num_filters*2)\n",
    "    upsample_6 = upsample_block(upsample_5, down_sample_1, num_filters)\n",
    "    upsample_7 = UpSampling2D(size=2)(upsample_6)\n",
    "    output_img = Conv2D(channels,kernel_size=4,strides=1,padding='same',activation='tanh')(upsample_7)\n",
    "    return Model(input_layer, output_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape,num_filters=64):\n",
    "    \"\"\" discrimitator block \"\"\"\n",
    "    def discriminator_block(incoming_layer,num_filters,kernel_size = 4,instance_normalization=True):\n",
    "        disc_layer = Conv2D(num_filters,kernel_size=kernel_size,strides=2,padding='same')(incoming_layer)\n",
    "        disc_layer = LeakyReLU(alpha = 0.2)(disc_layer)\n",
    "        if instance_normalization:\n",
    "            disc_layer = InstanceNormalization()(disc_layer)\n",
    "        return disc_layer\n",
    "\n",
    "    input_layer = Input(shape=img_shape)\n",
    "    #first layer not normalized\n",
    "    disc_block_1 = discriminator_block(input_layer,num_filters,instance_normalization=False)\n",
    "    disc_block_2 = discriminator_block(disc_block_1, num_filters*2)\n",
    "    disc_block_3 = discriminator_block(disc_block_2, num_filters*4)\n",
    "    disc_block_4 = discriminator_block(disc_block_3, num_filters*8)\n",
    "    output = Conv2D(1, kernel_size=4, strides=1, padding='same')(disc_block_4)\n",
    "    \n",
    "    return Model(input_layer, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Shape=(8, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "generator_filters = 32\n",
    "discriminator_filters = 64\n",
    "\n",
    "# input shape\n",
    "channels = 3\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, channels)\n",
    "# Loss weights\n",
    "lambda_cycle = 10.0\n",
    "\n",
    "lambda_identity = 0.1 * lambda_cycle\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# prepare patch size for our setup\n",
    "patch = int(IMG_HEIGHT / 2**4)\n",
    "patch_gan_shape = (patch, patch, 1)\n",
    "print(\"Patch Shape={}\".format(patch_gan_shape))\n",
    "\n",
    "# Discriminators\n",
    "disc_A = build_discriminator(input_shape,discriminator_filters)\n",
    "disc_A.compile(loss='mse',optimizer=optimizer,metrics=['accuracy'])\n",
    "disc_B = build_discriminator(input_shape,discriminator_filters)\n",
    "disc_B.compile(loss='mse',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "# Generators\n",
    "gen_AB = build_generator(input_shape,channels, generator_filters)\n",
    "gen_BA = build_generator(input_shape, channels, generator_filters)\n",
    "\n",
    "# CycleGAN\n",
    "img_A = Input(shape=input_shape)\n",
    "img_B = Input(shape=input_shape)\n",
    "\n",
    "# generate fake samples from both generators\n",
    "fake_B = gen_AB(img_A)\n",
    "fake_A = gen_BA(img_B)\n",
    "\n",
    "# reconstruct original samples from both generators\n",
    "reconstruct_A = gen_BA(fake_B)\n",
    "reconstruct_B = gen_AB(fake_A)\n",
    "\n",
    "# generate identity samples\n",
    "identity_A = gen_BA(img_A)\n",
    "identity_B = gen_AB(img_B)\n",
    "\n",
    "# disable discriminator training\n",
    "disc_A.trainable = False\n",
    "disc_B.trainable = False\n",
    "\n",
    "# use discriminator to classify real vs fake\n",
    "output_A = disc_A(fake_A)\n",
    "output_B = disc_B(fake_B)\n",
    "\n",
    "# Combined model trains generators to fool discriminators\n",
    "gan = Model(inputs=[img_A, img_B],\n",
    "outputs=[output_A, output_B,reconstruct_A, reconstruct_B,identity_A, identity_B ])\n",
    "\n",
    "gan.compile(loss=['mse', 'mse','mae', 'mae','mae', 'mae'],loss_weights=[1, 1,lambda_cycle, lambda_cycle,lambda_identity, lambda_identity ],optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that generates a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(path,\n",
    "                    batch_size=1,\n",
    "                    image_res=[128, 128],\n",
    "                    is_testing=False):\n",
    "    \"\"\"\n",
    "    Method to generate batch of images\n",
    "    Parameters:\n",
    "        path: type:str. Path to the dataset\n",
    "        batch_size: type:int. Number of images required\n",
    "        image_res: type:int list. Array denoting the resized [H,W] of image\n",
    "        is_testing: type: bool. Flag to control random flipping\n",
    "    Returns:\n",
    "        yields a tuple of two lists (source,target)\n",
    "    \"\"\"\n",
    "    data_type = \"train\" if not is_testing else \"test\"\n",
    "    path_A = glob('{}/{}A/*'.format(path, data_type))\n",
    "    path_B = glob('{}/{}B/*'.format(path, data_type))\n",
    "\n",
    "    num_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "    num_samples = num_batches * batch_size\n",
    "\n",
    "    # get num_samples from each domain\n",
    "    path_A = np.random.choice(path_A, num_samples, replace=False)\n",
    "    path_B = np.random.choice(path_B, num_samples, replace=False)\n",
    "\n",
    "    for i in range(num_batches-1):\n",
    "        batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "        batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A, imgs_B = [], []\n",
    "        for img_A, img_B in zip(batch_A, batch_B):\n",
    "            img_A = imread(img_A, image_res)\n",
    "            img_B = imread(img_B, image_res)\n",
    "\n",
    "            if not is_testing and np.random.random() > 0.5:\n",
    "                img_A = np.fliplr(img_A)\n",
    "                img_B = np.fliplr(img_B)\n",
    "\n",
    "            imgs_A.append(img_A)\n",
    "            imgs_B.append(img_B)\n",
    "\n",
    "        imgs_A = np.array(imgs_A)\n",
    "        imgs_B = np.array(imgs_B)\n",
    "\n",
    "        yield imgs_A, imgs_B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gen_AB,gen_BA,disc_A,disc_B,gan,patch_gan_shape,epochs,\n",
    "            path='/content/{}'.format(dataset_name),batch_size=1,sample_interval=50):\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "    real_y = np.ones((batch_size,) + patch_gan_shape)\n",
    "    fake_y = np.zeros((batch_size,) + patch_gan_shape)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch={}\".format(epoch))\n",
    "        for idx, (imgs_A, imgs_B) in enumerate(batch_generator(path,batch_size,image_res=[IMG_HEIGHT, IMG_WIDTH])):\n",
    "            \n",
    "            # train discriminators\n",
    "            # generate fake samples from both generators\n",
    "            fake_B = gen_AB.predict(imgs_A)\n",
    "            fake_A = gen_BA.predict(imgs_B)\n",
    "            # Train the discriminators\n",
    "            # (original images = real / translated = Fake)\n",
    "            disc_A_loss_real = disc_A.train_on_batch(imgs_A, real_y)\n",
    "            disc_A_loss_fake = disc_A.train_on_batch(fake_A, fake_y)\n",
    "            disc_A_loss = 0.5 * np.add(disc_A_loss_real,disc_A_loss_fake)\n",
    "\n",
    "            disc_B_loss_real = disc_B.train_on_batch(imgs_B, real_y)\n",
    "            disc_B_loss_fake = disc_B.train_on_batch(fake_B, fake_y)\n",
    "            disc_B_loss = 0.5 * np.add(disc_B_loss_real,disc_B_loss_fake)\n",
    "            \n",
    "            # Total disciminator loss\n",
    "            discriminator_loss = 0.5 * np.add(disc_A_loss, disc_B_loss)\n",
    "\n",
    "            # train generator\n",
    "            gen_loss = gan.train_on_batch([imgs_A, imgs_B],[real_y, real_y,imgs_A, imgs_B,imgs_A, imgs_B])\n",
    "\n",
    "            # training updates every 50 iterations\n",
    "            if idx % 50 == 0:\n",
    "                print (\"[Epoch {}/{}] [Discriminator loss: {}, accuracy:{}][Generator loss: {}, Adversarial Loss: {}, Reconstruction Loss: {},Identity Loss: {}]\".format(idx,epoch,discriminator_loss[0],100*discriminator_loss[1],\n",
    "                        gen_loss[0],np.mean(gen_loss[1:3]),np.mean(gen_loss[3:5]),np.mean(gen_loss[5:6])))\n",
    "        \n",
    "            # Plot and Save progress every few iterations\n",
    "            if idx % sample_interval == 0:\n",
    "                plot_sample_images(gen_AB,gen_BA,path=path,epoch=epoch,batch_num=idx,output_dir='images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/content\\\\apple2orange.tar.gz'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.get_file('{}.tar.gz'.format(dataset_name),\n",
    "                         origin=DOWNLOAD_URL,\n",
    "                         cache_subdir='/content',\n",
    "                         extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0\n",
      "Epoch=1\n",
      "Epoch=2\n",
      "Epoch=3\n",
      "Epoch=4\n",
      "Epoch=5\n",
      "Epoch=6\n",
      "Epoch=7\n",
      "Epoch=8\n",
      "Epoch=9\n",
      "Epoch=10\n",
      "Epoch=11\n",
      "Epoch=12\n",
      "Epoch=13\n",
      "Epoch=14\n",
      "Epoch=15\n",
      "Epoch=16\n",
      "Epoch=17\n",
      "Epoch=18\n",
      "Epoch=19\n",
      "Epoch=20\n",
      "Epoch=21\n",
      "Epoch=22\n",
      "Epoch=23\n",
      "Epoch=24\n",
      "Epoch=25\n",
      "Epoch=26\n",
      "Epoch=27\n",
      "Epoch=28\n",
      "Epoch=29\n",
      "Epoch=30\n",
      "Epoch=31\n",
      "Epoch=32\n",
      "Epoch=33\n",
      "Epoch=34\n",
      "Epoch=35\n",
      "Epoch=36\n",
      "Epoch=37\n",
      "Epoch=38\n",
      "Epoch=39\n",
      "Epoch=40\n",
      "Epoch=41\n",
      "Epoch=42\n",
      "Epoch=43\n",
      "Epoch=44\n",
      "Epoch=45\n",
      "Epoch=46\n",
      "Epoch=47\n",
      "Epoch=48\n",
      "Epoch=49\n",
      "Epoch=50\n",
      "Epoch=51\n",
      "Epoch=52\n",
      "Epoch=53\n",
      "Epoch=54\n",
      "Epoch=55\n",
      "Epoch=56\n",
      "Epoch=57\n",
      "Epoch=58\n",
      "Epoch=59\n",
      "Epoch=60\n",
      "Epoch=61\n",
      "Epoch=62\n",
      "Epoch=63\n",
      "Epoch=64\n",
      "Epoch=65\n",
      "Epoch=66\n",
      "Epoch=67\n",
      "Epoch=68\n",
      "Epoch=69\n",
      "Epoch=70\n",
      "Epoch=71\n",
      "Epoch=72\n",
      "Epoch=73\n",
      "Epoch=74\n",
      "Epoch=75\n",
      "Epoch=76\n",
      "Epoch=77\n",
      "Epoch=78\n",
      "Epoch=79\n",
      "Epoch=80\n",
      "Epoch=81\n",
      "Epoch=82\n",
      "Epoch=83\n",
      "Epoch=84\n",
      "Epoch=85\n",
      "Epoch=86\n",
      "Epoch=87\n",
      "Epoch=88\n",
      "Epoch=89\n",
      "Epoch=90\n",
      "Epoch=91\n",
      "Epoch=92\n",
      "Epoch=93\n",
      "Epoch=94\n",
      "Epoch=95\n",
      "Epoch=96\n",
      "Epoch=97\n",
      "Epoch=98\n",
      "Epoch=99\n",
      "Epoch=100\n",
      "Epoch=101\n",
      "Epoch=102\n",
      "Epoch=103\n",
      "Epoch=104\n",
      "Epoch=105\n",
      "Epoch=106\n",
      "Epoch=107\n",
      "Epoch=108\n",
      "Epoch=109\n",
      "Epoch=110\n",
      "Epoch=111\n",
      "Epoch=112\n",
      "Epoch=113\n",
      "Epoch=114\n",
      "Epoch=115\n",
      "Epoch=116\n",
      "Epoch=117\n",
      "Epoch=118\n",
      "Epoch=119\n",
      "Epoch=120\n",
      "Epoch=121\n",
      "Epoch=122\n",
      "Epoch=123\n",
      "Epoch=124\n",
      "Epoch=125\n",
      "Epoch=126\n",
      "Epoch=127\n",
      "Epoch=128\n",
      "Epoch=129\n",
      "Epoch=130\n",
      "Epoch=131\n",
      "Epoch=132\n",
      "Epoch=133\n",
      "Epoch=134\n",
      "Epoch=135\n",
      "Epoch=136\n",
      "Epoch=137\n",
      "Epoch=138\n",
      "Epoch=139\n",
      "Epoch=140\n",
      "Epoch=141\n",
      "Epoch=142\n",
      "Epoch=143\n",
      "Epoch=144\n",
      "Epoch=145\n",
      "Epoch=146\n",
      "Epoch=147\n",
      "Epoch=148\n",
      "Epoch=149\n",
      "Epoch=150\n",
      "Epoch=151\n",
      "Epoch=152\n",
      "Epoch=153\n",
      "Epoch=154\n",
      "Epoch=155\n",
      "Epoch=156\n",
      "Epoch=157\n",
      "Epoch=158\n",
      "Epoch=159\n",
      "Epoch=160\n",
      "Epoch=161\n",
      "Epoch=162\n",
      "Epoch=163\n",
      "Epoch=164\n",
      "Epoch=165\n",
      "Epoch=166\n",
      "Epoch=167\n",
      "Epoch=168\n",
      "Epoch=169\n",
      "Epoch=170\n",
      "Epoch=171\n",
      "Epoch=172\n",
      "Epoch=173\n",
      "Epoch=174\n",
      "Epoch=175\n",
      "Epoch=176\n",
      "Epoch=177\n",
      "Epoch=178\n",
      "Epoch=179\n",
      "Epoch=180\n",
      "Epoch=181\n",
      "Epoch=182\n",
      "Epoch=183\n",
      "Epoch=184\n",
      "Epoch=185\n",
      "Epoch=186\n",
      "Epoch=187\n",
      "Epoch=188\n",
      "Epoch=189\n",
      "Epoch=190\n",
      "Epoch=191\n",
      "Epoch=192\n",
      "Epoch=193\n",
      "Epoch=194\n",
      "Epoch=195\n",
      "Epoch=196\n",
      "Epoch=197\n",
      "Epoch=198\n",
      "Epoch=199\n"
     ]
    }
   ],
   "source": [
    "train(gen_AB, \n",
    "      gen_BA, \n",
    "      disc_A, \n",
    "      disc_B, \n",
    "      gan, \n",
    "      patch_gan_shape, \n",
    "      epochs=200, \n",
    "      batch_size=1, \n",
    "      sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3d0e8e0c817ae3c7f78f66b04bafeb63523a0a50e6405ec1b074461715681e4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
